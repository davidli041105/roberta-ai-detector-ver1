import streamlit as st
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import os

# ================= é¡µé¢é…ç½® =================
st.set_page_config(page_title="AI æ–‡æœ¬æ£€æµ‹å™¨", page_icon="ğŸ•µï¸")
st.title("ğŸ•µï¸ AI æ–‡æœ¬ç”Ÿæˆæ£€æµ‹ (RoBERTa + LoRA)")
st.markdown("åŸºäº RoBERTa-base å¾®è°ƒæ¨¡å‹ï¼Œåˆ¤æ–­æ–‡æœ¬æ˜¯ **äººç±»æ’°å†™** è¿˜æ˜¯ **AI ç”Ÿæˆ**ã€‚")

# ================= æ¨¡å‹åŠ è½½é€»è¾‘ =================

# ä½ çš„ LoRA æƒé‡æ–‡ä»¶å¤¹å (å¿…é¡»å’Œä¸Šä¼ åˆ° GitHub çš„æ–‡ä»¶å¤¹åä¸€è‡´)
LORA_PATH = "final_lora_model" 

@st.cache_resource
def load_model():
    """
    åŠ è½½æ¨¡å‹å‡½æ•°ï¼Œä½¿ç”¨ç¼“å­˜é¿å…æ¯æ¬¡é¢„æµ‹éƒ½é‡æ–°ä¸‹è½½
    """
    print("æ­£åœ¨åŠ è½½é…ç½®...")
    
    # 1. åŠ è½½ LoRA é…ç½®
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªè¯»å–é…ç½®ï¼Œä¸ç›´æ¥åŠ è½½æ¨¡å‹
    if not os.path.exists(LORA_PATH):
        st.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹: {LORA_PATH}ï¼Œè¯·æ£€æŸ¥ GitHub ä»“åº“ç»“æ„")
        return None, None
        
    config = PeftConfig.from_pretrained(LORA_PATH)
    
    # 2. ç¡®å®šåŸºåº§æ¨¡å‹åç§°
    # å…³é”®ä¿®æ”¹ï¼šäº‘ç«¯æ²¡æœ‰ '/root/autodl-tmp/...' è¿™ç§è·¯å¾„ã€‚
    # æˆ‘ä»¬å¼ºåˆ¶å°†å…¶æŒ‡å‘ Hugging Face å®˜æ–¹æ¨¡å‹ IDã€‚
    # å¦‚æœä½ ç”¨çš„æ˜¯ roberta-baseï¼Œè¿™é‡Œå†™ "roberta-base"
    # å¦‚æœæ˜¯ä¸­æ–‡ robertaï¼Œå¯èƒ½æ˜¯ "hfl/chinese-roberta-wwm-ext"
    base_model_name = "roberta-base" 
    
    print(f"æ­£åœ¨ä» HuggingFace ä¸‹è½½åŸºåº§æ¨¡å‹: {base_model_name}...")
    
    # 3. åŠ è½½åŸºåº§æ¨¡å‹ (ä»ç½‘ç»œä¸‹è½½)
    base_model = AutoModelForSequenceClassification.from_pretrained(
        base_model_name,
        num_labels=2, # ä¿æŒå’Œä½ è®­ç»ƒæ—¶ä¸€è‡´
        ignore_mismatched_sizes=True 
    )
    
    # 4. åŠ è½½åˆ†è¯å™¨ (Tokenizer)
    # ä¼˜å…ˆå°è¯•ä» LoRA æ–‡ä»¶å¤¹åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»åŸºåº§åŠ è½½
    try:
        tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)
    except:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)

    # 5. åˆå¹¶ LoRA æƒé‡
    print("æ­£åœ¨åˆå¹¶ LoRA æƒé‡...")
    inference_model = PeftModel.from_pretrained(base_model, LORA_PATH)
    
    # 6. è®¾å¤‡é…ç½® (Streamlit Cloud åªæœ‰ CPUï¼Œæ‰€ä»¥è¿™é‡Œè‡ªåŠ¨åˆ¤æ–­)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inference_model.to(device)
    inference_model.eval()
    
    return inference_model, tokenizer, device

# ================= UI äº¤äº’ä¸æ¨ç† =================

# æ˜¾ç¤ºåŠ è½½çŠ¶æ€
with st.spinner('æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ï¼Œåˆæ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½åŸºåº§æ¨¡å‹ (çº¦500MB)...'):
    try:
        model, tokenizer, device = load_model()
        if model:
            st.success("æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        st.stop()

# è¾“å…¥åŒºåŸŸ
text_input = st.text_area("è¯·è¾“å…¥è¦æ£€æµ‹çš„è‹±æ–‡æ–‡æœ¬ï¼š", height=200, placeholder="Type something here...")

if st.button("å¼€å§‹æ£€æµ‹", type="primary"):
    if not text_input.strip():
        st.warning("è¯·å…ˆè¾“å…¥å†…å®¹ï¼")
    else:
        # æ•°æ®é¢„å¤„ç†
        inputs = tokenizer(
            text_input, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        ).to(device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
            # è·å–æ¦‚ç‡
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            # è·å–æœ€å¤§æ¦‚ç‡çš„æ ‡ç­¾ç´¢å¼•
            pred_label = torch.argmax(probs, dim=-1).item()
            
            # è·å– AI (æ ‡ç­¾1) çš„æ¦‚ç‡
            ai_probability = probs[0][1].item()
            human_probability = probs[0][0].item()

        # ç»“æœå±•ç¤º
        st.divider()
        
        # é€»è¾‘ï¼šæ ‡ç­¾ 1 = AI, æ ‡ç­¾ 0 = äººç±»
        if pred_label == 1:
            st.error("ğŸ¤– æ£€æµ‹ç»“æœï¼šAI ç”Ÿæˆ")
            st.progress(ai_probability)
            st.write(f"**AI æ¦‚ç‡:** {ai_probability:.2%}")
        else:
            st.success("ğŸ§‘ æ£€æµ‹ç»“æœï¼šäººç±»æ’°å†™")
            st.progress(human_probability)
            st.write(f"**äººç±»æ¦‚ç‡:** {human_probability:.2%}")

# debug ä¿¡æ¯ (å¯é€‰)
with st.expander("æŸ¥çœ‹è¯¦ç»†æ¦‚ç‡"):
    if 'probs' in locals():
        st.json({
            "Human_Label_0": f"{probs[0][0].item():.4f}",
            "AI_Label_1": f"{probs[0][1].item():.4f}"
        })